{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Skill Extraction Testing\n",
                "\n",
                "This notebook prototypes and tests NLP-based skill extraction.\n",
                "\n",
                "## Objectives\n",
                "- Test skill extraction accuracy\n",
                "- Tune confidence thresholds\n",
                "- Explore spaCy NLP features\n",
                "- Evaluate taxonomy coverage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import json\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Load spaCy\n",
                "import spacy\n",
                "nlp = spacy.load('en_core_web_md')\n",
                "print(f\"Loaded spaCy model: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
                "print(f\"Vector size: {nlp.vocab.vectors_length}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Skill Extraction from Sample Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.feature_engineering.skill_extractor import extract_skills, load_skills_taxonomy\n",
                "\n",
                "# Load taxonomy\n",
                "taxonomy = load_skills_taxonomy()\n",
                "all_skills = [skill for skills in taxonomy.values() for skill in skills]\n",
                "print(f\"Total skills in taxonomy: {len(all_skills)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test samples\n",
                "test_texts = [\n",
                "    \"Senior Python developer with experience in Django, Flask, and machine learning using TensorFlow.\",\n",
                "    \"Full-stack engineer skilled in React, Node.js, and AWS. Strong leadership and communication skills.\",\n",
                "    \"Data scientist proficient in pandas, numpy, scikit-learn. Experience in healthcare and finance domains.\",\n",
                "    \"DevOps engineer with expertise in Docker, Kubernetes, and CI/CD pipelines using Jenkins.\"\n",
                "]\n",
                "\n",
                "for i, text in enumerate(test_texts, 1):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Test {i}: {text[:60]}...\")\n",
                "    print('='*60)\n",
                "    \n",
                "    skills = extract_skills(text, taxonomy)\n",
                "    \n",
                "    # Group by category\n",
                "    by_category = {'technical': [], 'soft': [], 'domain': []}\n",
                "    for skill in skills:\n",
                "        by_category[skill['category']].append(f\"{skill['skill_name']} ({skill['confidence']:.0%})\")\n",
                "    \n",
                "    for category, skill_list in by_category.items():\n",
                "        if skill_list:\n",
                "            print(f\"  {category.upper()}: {', '.join(skill_list)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Confidence Threshold Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different confidence thresholds\n",
                "sample_text = \"\"\"\n",
                "Experienced software engineer with Python, JavaScript, and Java skills.\n",
                "Proficient in React and Django. Some knowledge of AWS and Docker.\n",
                "Good communication and teamwork abilities.\n",
                "\"\"\"\n",
                "\n",
                "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "results = []\n",
                "\n",
                "for threshold in thresholds:\n",
                "    skills = extract_skills(sample_text, taxonomy, min_confidence=threshold)\n",
                "    results.append({\n",
                "        'threshold': threshold,\n",
                "        'count': len(skills),\n",
                "        'skills': [s['skill_name'] for s in skills]\n",
                "    })\n",
                "    print(f\"Threshold {threshold}: {len(skills)} skills - {[s['skill_name'] for s in skills]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize threshold impact\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.bar([r['threshold'] for r in results], [r['count'] for r in results], color='steelblue')\n",
                "plt.xlabel('Confidence Threshold')\n",
                "plt.ylabel('Number of Skills Extracted')\n",
                "plt.title('Impact of Confidence Threshold on Skill Extraction')\n",
                "plt.xticks(thresholds)\n",
                "for i, r in enumerate(results):\n",
                "    plt.text(r['threshold'], r['count'] + 0.2, str(r['count']), ha='center', fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. SpaCy NLP Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore spaCy features\n",
                "doc = nlp(sample_text)\n",
                "\n",
                "# Named entities\n",
                "print(\"Named Entities:\")\n",
                "for ent in doc.ents:\n",
                "    print(f\"  {ent.text} ({ent.label_})\")\n",
                "\n",
                "print(\"\\nNoun Phrases:\")\n",
                "for chunk in doc.noun_chunks:\n",
                "    print(f\"  {chunk.text}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word vectors similarity\n",
                "skills_to_test = ['Python', 'JavaScript', 'programming', 'coding']\n",
                "\n",
                "print(\"Word Similarity Matrix:\")\n",
                "for skill1 in skills_to_test:\n",
                "    sims = []\n",
                "    for skill2 in skills_to_test:\n",
                "        sim = nlp(skill1).similarity(nlp(skill2))\n",
                "        sims.append(f\"{sim:.2f}\")\n",
                "    print(f\"  {skill1:12}: {', '.join(sims)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Accuracy Metrics\n",
                "\n",
                "Based on v0.1 requirements:\n",
                "- Precision ≥70%\n",
                "- Recall ≥60%\n",
                "- Categorization accuracy ≥80%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with labeled ground truth\n",
                "test_case = {\n",
                "    'text': 'Python developer with React experience and strong communication skills.',\n",
                "    'expected_skills': ['Python', 'React', 'Communication']\n",
                "}\n",
                "\n",
                "extracted = extract_skills(test_case['text'], taxonomy)\n",
                "extracted_names = [s['skill_name'] for s in extracted]\n",
                "\n",
                "# Calculate metrics\n",
                "true_positives = len(set(extracted_names) & set(test_case['expected_skills']))\n",
                "precision = true_positives / len(extracted_names) if extracted_names else 0\n",
                "recall = true_positives / len(test_case['expected_skills']) if test_case['expected_skills'] else 0\n",
                "\n",
                "print(f\"Expected: {test_case['expected_skills']}\")\n",
                "print(f\"Extracted: {extracted_names}\")\n",
                "print(f\"\\nPrecision: {precision:.0%}\")\n",
                "print(f\"Recall: {recall:.0%}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}