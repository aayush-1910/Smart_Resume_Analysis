{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 - Text Extraction Experiments\n",
                "\n",
                "This notebook tests and compares different PDF text extraction methods.\n",
                "\n",
                "## Objectives\n",
                "- Compare pdfplumber vs PyPDF2 extraction\n",
                "- Test multi-column layout handling\n",
                "- Measure extraction accuracy\n",
                "- Handle edge cases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import os\n",
                "from pathlib import Path\n",
                "import time\n",
                "\n",
                "# Import extraction modules\n",
                "from src.preprocessing.pdf_extractor import extract_text_from_pdf, PDFExtractionError"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Test PDF Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List available PDF files\n",
                "raw_data_dir = Path('../data/raw')\n",
                "pdf_files = list(raw_data_dir.glob('*.pdf'))\n",
                "\n",
                "print(f\"Found {len(pdf_files)} PDF files:\")\n",
                "for pdf in pdf_files:\n",
                "    print(f\"  - {pdf.name}\")\n",
                "\n",
                "if not pdf_files:\n",
                "    print(\"\\n⚠️ No PDF files found. Add sample resumes to data/raw/ directory.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to test extraction with both methods\n",
                "def compare_extraction_methods(pdf_path):\n",
                "    results = {}\n",
                "    \n",
                "    # Test pdfplumber\n",
                "    try:\n",
                "        start = time.time()\n",
                "        result_plumber = extract_text_from_pdf(pdf_path, method='pdfplumber')\n",
                "        results['pdfplumber'] = {\n",
                "            'time': time.time() - start,\n",
                "            'text_length': len(result_plumber['text']),\n",
                "            'pages': result_plumber['num_pages'],\n",
                "            'success': result_plumber['success'],\n",
                "            'preview': result_plumber['text'][:500]\n",
                "        }\n",
                "    except Exception as e:\n",
                "        results['pdfplumber'] = {'error': str(e)}\n",
                "    \n",
                "    # Test PyPDF2\n",
                "    try:\n",
                "        start = time.time()\n",
                "        result_pypdf2 = extract_text_from_pdf(pdf_path, method='pypdf2')\n",
                "        results['pypdf2'] = {\n",
                "            'time': time.time() - start,\n",
                "            'text_length': len(result_pypdf2['text']),\n",
                "            'pages': result_pypdf2['num_pages'],\n",
                "            'success': result_pypdf2['success'],\n",
                "            'preview': result_pypdf2['text'][:500]\n",
                "        }\n",
                "    except Exception as e:\n",
                "        results['pypdf2'] = {'error': str(e)}\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run comparison on available PDFs\n",
                "if pdf_files:\n",
                "    for pdf in pdf_files[:3]:  # Test first 3 PDFs\n",
                "        print(f\"\\n{'='*50}\")\n",
                "        print(f\"Testing: {pdf.name}\")\n",
                "        print('='*50)\n",
                "        \n",
                "        results = compare_extraction_methods(str(pdf))\n",
                "        \n",
                "        for method, data in results.items():\n",
                "            print(f\"\\n{method.upper()}:\")\n",
                "            if 'error' in data:\n",
                "                print(f\"  Error: {data['error']}\")\n",
                "            else:\n",
                "                print(f\"  Time: {data['time']:.2f}s\")\n",
                "                print(f\"  Text Length: {data['text_length']} chars\")\n",
                "                print(f\"  Pages: {data['pages']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Cleaning Tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.preprocessing.text_cleaner import clean_text, remove_headers_footers\n",
                "\n",
                "# Test text with various issues\n",
                "messy_text = \"\"\"\n",
                "Page 1 of 3\n",
                "\n",
                "John    Doe\n",
                "Software      Engineer\n",
                "\n",
                "\n",
                "\n",
                "Email:    john@example.com\n",
                "\n",
                "Page 2 of 3\n",
                "\n",
                "EXPERIENCE\n",
                "Company ABC    —    Senior Developer\n",
                "\n",
                "Confidential\n",
                "\"\"\"\n",
                "\n",
                "# Clean text\n",
                "cleaned = clean_text(messy_text)\n",
                "print(\"After clean_text():\")\n",
                "print(cleaned)\n",
                "\n",
                "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
                "\n",
                "# Remove headers/footers\n",
                "no_headers = remove_headers_footers(messy_text)\n",
                "print(\"After remove_headers_footers():\")\n",
                "print(no_headers)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Edge Case Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test error handling\n",
                "test_cases = [\n",
                "    ('nonexistent.pdf', 'File not found'),\n",
                "    (None, 'None input'),\n",
                "]\n",
                "\n",
                "for test_path, description in test_cases:\n",
                "    print(f\"\\nTest: {description}\")\n",
                "    try:\n",
                "        result = extract_text_from_pdf(test_path)\n",
                "        print(f\"  Result: {result}\")\n",
                "    except Exception as e:\n",
                "        print(f\"  Exception: {type(e).__name__}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Performance Metrics\n",
                "\n",
                "Based on v0.1 requirements:\n",
                "- PDF extraction accuracy ≥95% (single-column)\n",
                "- Multi-column accuracy ≥80%\n",
                "- Email detection ≥90%\n",
                "- Processing speed ≤5s per resume"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Placeholder for accuracy metrics\n",
                "# TODO: Add labeled test data and calculate metrics\n",
                "\n",
                "metrics = {\n",
                "    'single_column_accuracy': 'TBD',\n",
                "    'multi_column_accuracy': 'TBD',\n",
                "    'email_detection_rate': 'TBD',\n",
                "    'avg_processing_time': 'TBD'\n",
                "}\n",
                "\n",
                "print(\"Performance Metrics (to be calculated with test data):\")\n",
                "for metric, value in metrics.items():\n",
                "    print(f\"  {metric}: {value}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}